{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Optimization\n",
    "\n",
    "In this notebook hyper-parameters of XGBoost are optimized for [HIGGS](https://archive.ics.uci.edu/ml/datasets/HIGGS) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.1)\n",
    "s = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, [scikit-optimize](https://scikit-optimize.github.io/) (skopt) library is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from skopt import Optimizer\n",
    "\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import Matern, RBF, WhiteKernel\n",
    "\n",
    "from skopt.learning import RandomForestRegressor\n",
    "\n",
    "from skopt.acquisition import gaussian_ei as acq_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The original dataset is quite large, thus, in order to save time, only a portion of it is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT='/mnt/mlhep2018/datasets/'\n",
    "\n",
    "SAMPLE_LIMIT=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "data = np.genfromtxt(\n",
    "    osp.join(DATA_ROOT, 'HIGGS.csv'),\n",
    "    dtype='float32',\n",
    "    delimiter=',',\n",
    "    max_rows=SAMPLE_LIMIT\n",
    ")\n",
    "\n",
    "X, y = data[:, 1:], data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### again to save training time only 10% of the loaded data is used for training.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# want to split data set into three sections\n",
    "# one for training hyperparameters\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max depth tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes we perform Baysian Optimization on 1 parameter: `learning rate`.\n",
    "\n",
    "Learning rate in XGBoost can vary dramatically by orders of magnitude, thus, we use *logarithmic scale*.\n",
    "\n",
    "The goal is to maximize `ROC AUC` (minimize 1 - `ROC AUC`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_1(log_learning_rate, X_train=X_train, y_train=y_train, X_score=X_test, y_score=y_test):\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=3, \n",
    "        n_estimators=50,\n",
    "        learning_rate=np.exp(log_learning_rate),\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=20,\n",
    "        reg_lambda=0.0\n",
    "    )\n",
    "\n",
    "    xgb.fit(X_train, y_train)\n",
    "    predictions = xgb.predict_proba(X_score)[:, 1]\n",
    "    \n",
    "    return 1 - roc_auc_score(y_score, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### learning rate <- [1.0e-3, 1]\n",
    "dimensions_1 = [\n",
    "    (np.log(1.0e-3), 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "target_function_1(0.0, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### utility function to inspect status of BO.\n",
    "\n",
    "def plot_bo(bo, suggestion=None, value=None):\n",
    "    a, b = bo.space.bounds[0]\n",
    "    \n",
    "    ### getting the latest model\n",
    "    model = bo.models[-1]\n",
    "    \n",
    "    xs = np.linspace(a, b, num=100)\n",
    "    x_model = bo.space.transform(xs.reshape(-1, 1).tolist())\n",
    "    \n",
    "    mean, std = model.predict(x_model, return_std=True)\n",
    "    \n",
    "    plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(\n",
    "        np.array(bo.Xi)[:, 0],\n",
    "        np.array(bo.yi),\n",
    "        color='red',\n",
    "        label='observations'\n",
    "    )\n",
    "    if suggestion is not None:\n",
    "        plt.scatter([suggestion], value, color='blue', label='suggestion')\n",
    "    \n",
    "    plt.plot(xs, mean, color='green', label='model')\n",
    "    plt.fill_between(xs, mean - 1.96 * std, mean + 1.96 * std, alpha=0.1, color='green')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    acq = acq_func(x_model, model, np.min(bo.yi))\n",
    "    plt.plot(xs, acq, label='Expected Improvement')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### another utility function\n",
    "def cum_min(xs):\n",
    "    result = np.zeros_like(xs)\n",
    "    cmin = xs[0]\n",
    "    \n",
    "    result[0] = xs[0]\n",
    "    \n",
    "    for i in range(1, xs.shape[0]):\n",
    "        if cmin > xs[i]:\n",
    "            cmin = xs[i]\n",
    "\n",
    "        result[i] = cmin\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "### plots progress of BO over time.\n",
    "\n",
    "def plot_convergence(bo):\n",
    "    display.clear_output(wait=True)\n",
    "    values = np.array(bo.yi)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cum_min(values), label='minimal discovered')\n",
    "    plt.scatter(np.arange(len(bo.yi)), bo.yi, label='observations')\n",
    "    plt.xlabel('step', fontsize=14)\n",
    "    plt.ylabel('loss', fontsize=14)\n",
    "    \n",
    "    plt.legend(loc='upper right', fontsize=18)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create an instance of Baysian Optimizer\n",
    "bo_gp_1 = Optimizer(\n",
    "    ### telling optimizer boundaries for each parameter\n",
    "    dimensions=dimensions_1,\n",
    "    \n",
    "    ### setting regressor\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-6, 1.0e+6]) + \\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2]),\n",
    "    ),\n",
    "    n_initial_points=2,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    x = bo_gp_1.ask()\n",
    "    f = target_function_1(x[0], X_train, y_train, X_val, y_val)\n",
    "\n",
    "    if len(bo_gp_1.models) > 0:\n",
    "        plot_bo(bo_gp_1, suggestion=x, value=f)\n",
    "    \n",
    "    bo_gp_1.tell(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(bo_gp_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmin(bo_gp_1.yi)\n",
    "best_parameters = bo_gp_1.Xi[best]\n",
    "\n",
    "print('Best log learning rate: %.2e' % best_parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 1 - target_function_1(best_parameters[0], X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Best ROC AUC: %.3lf' % best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the same but with random forest\n",
    "\n",
    "bo_rf_1 = Optimizer(\n",
    "    dimensions=dimensions_1,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=3,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    x = bo_rf_1.ask()\n",
    "    f = target_function_1(x[0])\n",
    "\n",
    "    if len(bo_rf_1.models) > 0:\n",
    "        plot_bo(bo_rf_1, suggestion=x, value=f)\n",
    "    \n",
    "    bo_rf_1.tell(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(bo_rf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmin(bo_rf_1.yi)\n",
    "best_parameters = bo_rf_1.Xi[best]\n",
    "\n",
    "print('Best log learning rate: %.2e' % best_parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 1 - target_function_1(best_parameters[0], X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Best ROC AUC: %.3lf' % best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning all important hyper-parameters simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_4(params, X_train, y_train, X_score, y_score):\n",
    "    \"\"\"\n",
    "    params - array with 4 values: [max tree depth, number of estimators, log learning rate, reg_lambda]\n",
    "    X_train, y_train - training dataset;\n",
    "    X_score, y_score - dataset for evaluating quality of the classifier;\n",
    "    \n",
    "    Returns 1 - ROC AUC evaluated on `X_score` and `y_score`. \n",
    "    \"\"\"\n",
    "    max_depth, n_estimators, log_learning_rate, reg_lambda = params\n",
    "\n",
    "    max_depth = int(np.ceil(max_depth))\n",
    "    n_estimators = int(np.ceil(n_estimators))\n",
    "    \n",
    "    learning_rate = np.exp(log_learning_rate)\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=4\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    predictions = xgb.predict_proba(X_score)[:, 1]\n",
    "    \n",
    "    return 1 - roc_auc_score(y_score, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best(bo):\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    max_depth, n_estimators, log_learning_rate, reg_lambda = best_parameters\n",
    "    \n",
    "    print(\n",
    "        'Best model:\\n\\nmax_depth=%d\\nn_estimators=%d\\nlearning_rate=%.1e\\nreg_lambda=%.3lf' % (\n",
    "            int(np.ceil(max_depth)),\n",
    "            int(np.ceil(n_estimators)),\n",
    "            np.exp(log_learning_rate),\n",
    "            reg_lambda\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    best_auc = target_function_4(best_parameters, X_train, y_train, X_test, y_test)\n",
    "    print('Best ROC AUC: %.3lf' % (1 - best_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_4 =[\n",
    "    ### max_depth\n",
    "    (1.0, 20.0),\n",
    "    \n",
    "    ### n_estimators\n",
    "    (1.0, 500.0),\n",
    "    \n",
    "    ### log_learning rate\n",
    "    (np.log(1.0e-3), np.log(1.0)),\n",
    "    \n",
    "    ### l2 reg\n",
    "    (0.0, 1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_gp_4 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-3, 1.0e+3]) +\\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2])\n",
    "    ),\n",
    "    n_initial_points=5,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    x = bo_gp_4.ask()\n",
    "    f = target_function_4(x, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    bo_gp_4.tell(x, f)\n",
    "    \n",
    "    plot_convergence(bo_gp_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best(bo_gp_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The same with Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_4 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=5,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    x = bo_rf_4.ask()\n",
    "    f = target_function_4(x, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    bo_rf_4.tell(x, f)\n",
    "    \n",
    "    plot_convergence(bo_rf_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best(bo_rf_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Optimization\n",
    "\n",
    "\n",
    "Sometimes it is preferable to have a good classifier with a particular configuration.\n",
    "\n",
    "For example, smaller ensemble is preferable to large ones (given the same quality), since small ensemble are faster to compute. One way to find preferable classifiers is to introduce a penalty to the target function:\n",
    "\n",
    "$$\\mathcal{L} = \\mathrm{quality\\;metric} + \\mathrm{penalty}$$\n",
    "\n",
    "\n",
    "For example, number of CPU operations per prediction can introduced as a penalty: \n",
    "$$\\mathcal{L} = \\mathrm{ROC\\; AUC} + C \\cdot \\mathrm{tree\\;depth} \\cdot \\mathrm{ensemble\\;size}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_4_struct(params, X_train, y_train, X_score, y_score):\n",
    "    \"\"\"\n",
    "    params - array with 4 values: [max tree depth, number of estimators, log learning rate, reg_lambda]\n",
    "    X_train, y_train - training dataset;\n",
    "    X_score, y_score - dataset for evaluating quality of the classifier;\n",
    "    \n",
    "    Returns 1 - ROC AUC evaluated on `X_score` and `y_score`. \n",
    "    \"\"\"\n",
    "    max_depth, n_estimators, log_learning_rate, reg_lambda = params\n",
    "\n",
    "    max_depth = int(np.ceil(max_depth))\n",
    "    n_estimators = int(np.ceil(n_estimators))\n",
    "    \n",
    "    learning_rate = np.exp(log_learning_rate)\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=4\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    predictions = xgb.predict_proba(X_score)[:, 1]\n",
    "    \n",
    "    score = 1 - roc_auc_score(y_score, predictions)\n",
    "    \n",
    "    penalty = 5.0e-5 * n_estimators * max_depth\n",
    "    \n",
    "    return score + penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_gp_4_struct = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-3, 1.0e+3]) +\\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2])\n",
    "    ),\n",
    "    n_initial_points=5,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    x = bo_gp_4_struct.ask()\n",
    "    f = target_function_4_struct(x, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    bo_gp_4_struct.tell(x, f)\n",
    "    \n",
    "    plot_convergence(bo_gp_4_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best(bo_gp_4_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Try to optimize neural network architecture for the HIGGS dataset.\n",
    "\n",
    "Your neural network **must** have less than $10^4$ parameters!\n",
    "\n",
    "Target metric: $\\mathrm{ROC\\;AUC} \\to \\max$\n",
    "\n",
    "**Normal problem**:\n",
    "- fix number of layers (e.g. 5);\n",
    "- choose type of the layers, non-linearity etc (e.g. `Dense(activation='relu')`);\n",
    "- optimize number of units in each layer;\n",
    "- introduce restrictions on number of parameters, either by:\n",
    "    - penalty e.g. `C * max(num_parameters - 1.0e+4, 0)`;\n",
    "    - scaling number of units until number of parameters drops to $10^4$;\n",
    "    - using fractions of total number of parameters for each layer (see `softmax` trick);\n",
    "- you may want to also include regularization coefficients (e.g. `alpha * l1 + beta * l2`).\n",
    "\n",
    "**Mad-data-scientist problem**:\n",
    "- try to optimize everything:\n",
    "    - find a way to encode variable number of layers (e.g. by introducing limit on number of layers);\n",
    "    - find a way to optimize activation function:\n",
    "        - e.g. by preselecting small set of activation functions and taking linear combination of them;\n",
    "        - skopt alse can deal with descrite variables (please, refer to the documentation of `dimensions` argument for optimizer);\n",
    "        - alternatively, you can introduce coeficients for each activation function (select 3-4) for each layers and split your layers according to these coefficients: `Concat(Dense(total_units * coef[0], activation='relu'), Dense(total_units * coef[1], activation='sigmoid'))`.\n",
    "\n",
    "**You can evaluate on the test set only once!**\n",
    "\n",
    "Tips:\n",
    "- try to optimize 1 parameter first, e.g. to select an appropriate kernel for GP, estimate noise level;\n",
    "-  `sigmoid` and `tanh` does not differ much (there is an easy-to-prove procedure that replaces one with another w/o changing the network);\n",
    "- if your want to have parameters that sum to one use `softmax` and fix one of the parameters to `1` (to remove overparametrization);\n",
    "- BO does not handle very well ordered descrete variables, it might be better to use `int(param)`:\n",
    "    - you might want to use cache in this case.\n",
    "- use small portion of the dataset e.g. `10^4` examples to accelerate search;\n",
    "- if you have symmetries in your parameterization, i.e. N parameter values correspond to the same network, you can report to BO all N parameters with the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "### don't forget about others!\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.1)\n",
    "tf_session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "keras.backend.tensorflow_backend.set_session(tf_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is next?\n",
    "\n",
    "- you can experiment with acqusition functions:\n",
    "    - compare expected improvement (`EI`) and lower confidence bound (`LCB`) to probability of improvement (`PI`);\n",
    "- explore different regressions available https://scikit-optimize.github.io/learning/index.html\n",
    "- try to shoot yourself in the foot by perfroming naive Bayesian inference on a Neural Network.\n",
    "- try partial Bayesian Inference with Neural Networks: https://arxiv.org/pdf/1502.05700.pdf\n",
    "- try REMBO-ing your optimum value: https://arxiv.org/abs/1301.1942\n",
    "\n",
    "<img src=\"https://imgix.ranker.com/user_node_img/50066/1001300261/original/there-was-a-big-crew-turnover-in-rambo-iii-photo-u1?w=650&q=50&fm=jpg&fit=crop&crop=faces\" width=454 height=193>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary building blocks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, InputLayer\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARAMS = int(1.0e+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the neural net\n",
    "def make_net(num_nodes1,num_nodes2,num_nodes3,num_nodes4):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=[X_train.shape[1]]))\n",
    "    model.add(Dense(num_nodes1, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(num_nodes2, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(num_nodes3, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(num_nodes4, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session= tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "keras.backend.tensorflow_backend.set_session(session)\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# num_nodes = [10, 10, 10, 10]\n",
    "# this_net = make_net(num_nodes[0],num_nodes[1],num_nodes[2],num_nodes[3])\n",
    "\n",
    "# this_net.summary()\n",
    "# this_net.compile(loss='mse', optimizer=keras.optimizers.adamax(lr=5e-3))\n",
    "# this_net.fit(X_train, y_train, epochs=1,batch_size=32)\n",
    "# predictions = this_net.predict(X_train)\n",
    "# print(np.unique(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make the NN with the given parameters\n",
    "\n",
    "def my_target_function(num_nodes, X_train=X_train, y_train=y_train, X_score=X_test, y_score=y_test):\n",
    "#     from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    K.clear_session()\n",
    "    \n",
    "#     xgb = XGBClassifier(\n",
    "#         max_depth=3, \n",
    "#         n_estimators=50,\n",
    "#         learning_rate=np.exp(log_learning_rate),\n",
    "#         objective='binary:logistic',\n",
    "#         n_jobs=20,\n",
    "#         reg_lambda=0.0\n",
    "#     )\n",
    "\n",
    "    this_net = make_net(num_nodes[0],num_nodes[1],num_nodes[2],num_nodes[3])\n",
    "    if this_net.count_params() > MAX_PARAMS:\n",
    "        return 0.5\n",
    "    else:\n",
    "        this_net.summary()\n",
    "        this_net.compile(loss='mse', optimizer=keras.optimizers.adamax(lr=5e-3))\n",
    "        this_net.fit(X_train, y_train, epochs=10,batch_size=32)\n",
    "        predictions = this_net.predict(X_score)\n",
    "        print(predictions)\n",
    "#         raise\n",
    "        return 1 - roc_auc_score(y_score, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dimensions = [\n",
    "    ## num_nodes range\n",
    "    (10,50),\n",
    "    (10,50),\n",
    "    (10,50),\n",
    "    (10,50)\n",
    "]\n",
    "\n",
    "# dimensions_4 =[\n",
    "#     ### max_depth\n",
    "#     (1.0, 20.0),\n",
    "    \n",
    "#     ### n_estimators\n",
    "#     (1.0, 500.0),\n",
    "    \n",
    "#     ### log_learning rate\n",
    "#     (np.log(1.0e-3), np.log(1.0)),\n",
    "    \n",
    "#     ### l2 reg\n",
    "#     (0.0, 1.0)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net_optimizer_struct = Optimizer(\n",
    "    dimensions=my_dimensions,\n",
    "#     base_estimator=GaussianProcessRegressor(\n",
    "#         kernel=RBF(length_scale_bounds=[1.0e-3, 1.0e+3]) +\\\n",
    "#             WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2])\n",
    "#     ),\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=Matern(nu=1.5) + WhiteKernel(\n",
    "            noise_level_bounds=(1.0e-3,1.0e-1))),\n",
    "    n_initial_points=3,\n",
    "    acq_func='LCB'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 23, 29, 13]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 14)                406       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 23)                345       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 29)                696       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 29)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 13)                390       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 1,851\n",
      "Trainable params: 1,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "FeedInputs: unable to find feed output input_1:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-da028957b336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_net_optimizer_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmy_net_optimizer_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1a64882ab028>\u001b[0m in \u001b[0;36mmy_target_function\u001b[0;34m(num_nodes, X_train, y_train, X_score, y_score)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mthis_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mthis_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mthis_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2633\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2635\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2636\u001b[0m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2585\u001b[0m         \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2587\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2588\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \"\"\"\n\u001b[1;32m   1482\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1439\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m             self._handle = tf_session.TF_DeprecatedSessionMakeCallable(\n\u001b[0;32m-> 1441\u001b[0;31m                 session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1442\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mlhep2018/pyenv/versions/3.6.6/envs/mlhep/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: FeedInputs: unable to find feed output input_1:0"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    x = my_net_optimizer_struct.ask()\n",
    "    print(x)\n",
    "    f = my_target_function(x, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    my_net_optimizer_struct.tell(x, f)\n",
    "    \n",
    "    plot_convergence(my_net_optimizer_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
